{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8468d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices:\n",
      " [   0    3    5    6   10   11   13 3299 3300 3301]\n",
      "Selected Features:\n",
      " Index(['Age', 'Customer_Support_Interactions', 'Customer_Satisfaction',\n",
      "       'Purchase_Frequency', 'Lifetime_Value', 'Average_Order_Value',\n",
      "       'Number_of_Product_Categories_Purchased',\n",
      "       'Loyalty_Program_Participation_Inactive',\n",
      "       'Engagement_with_Promotions_Low', 'Engagement_with_Promotions_Medium'],\n",
      "      dtype='object')\n",
      "\n",
      "Random Forest\n",
      "Confusion Matrix:\n",
      " [[17143    44]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9981493943472409\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "Confusion Matrix:\n",
      " [[17062   125]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9947425975773889\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     17187\n",
      "           1       0.98      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           0.99     23776\n",
      "   macro avg       0.99      1.00      0.99     23776\n",
      "weighted avg       0.99      0.99      0.99     23776\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Confusion Matrix:\n",
      " [[17147    40]\n",
      " [   26  6563]]\n",
      "Accuracy: 0.9972240915208613\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Confusion Matrix:\n",
      " [[17014   173]\n",
      " [    2  6587]]\n",
      "Accuracy: 0.9926396366083445\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     17187\n",
      "           1       0.97      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           0.99     23776\n",
      "   macro avg       0.99      0.99      0.99     23776\n",
      "weighted avg       0.99      0.99      0.99     23776\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "Confusion Matrix:\n",
      " [[17114    73]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9969296769851952\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       0.99      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Gradient Boosting\n",
      "Confusion Matrix:\n",
      " [[17137    50]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9978970390309556\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Confusion Matrix:\n",
      " [[15863  1324]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9443135935397039\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     17187\n",
      "           1       0.83      1.00      0.91      6589\n",
      "\n",
      "    accuracy                           0.94     23776\n",
      "   macro avg       0.92      0.96      0.93     23776\n",
      "weighted avg       0.95      0.94      0.95     23776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"cleaned_df_luxottica_churn_updated_0108.csv\", index_col=None)\n",
    "\n",
    "# Create a copy of the dataset for transformation\n",
    "dataset_transformed = pd.get_dummies(dataset, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataset_transformed.drop('Churn_Yes', axis=1)\n",
    "y = dataset_transformed['Churn_Yes']\n",
    "\n",
    "# Apply Min-Max scaling to ensure non-negative values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "\n",
    "# Initialize the SelectKBest with chi2\n",
    "select_k_best = SelectKBest(score_func=chi2, k=10)\n",
    "X_selected = select_k_best.fit_transform(X_scaled, y)\n",
    "\n",
    "# Get the selected feature indices and names\n",
    "selected_features_indices = select_k_best.get_support(indices=True)\n",
    "selected_features = X.columns[selected_features_indices]\n",
    "print(\"Selected Feature Indices:\\n\", selected_features_indices)\n",
    "print(\"Selected Features:\\n\", selected_features)\n",
    "\n",
    "# Create the final feature and target datasets with selected features\n",
    "X_final = X[selected_features]\n",
    "y_final = y\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.25, random_state=0)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary to store models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=0),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=0),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(random_state=0),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name}\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0be8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Cross-Validation Accuracy: 0.9981 (+/- 0.0003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Cross-Validation Accuracy: 0.9682 (+/- 0.0013)\n",
      "Decision Tree - Cross-Validation Accuracy: 0.9965 (+/- 0.0003)\n",
      "K-Nearest Neighbors - Cross-Validation Accuracy: 0.7520 (+/- 0.0007)\n",
      "Support Vector Machine - Cross-Validation Accuracy: 0.7361 (+/- 0.0010)\n",
      "Gradient Boosting - Cross-Validation Accuracy: 0.9980 (+/- 0.0003)\n",
      "Naive Bayes - Cross-Validation Accuracy: 0.9836 (+/- 0.0004)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate model using cross-validation\n",
    "for model_name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_final, y_final, cv=5)\n",
    "    print(f\"{model_name} - Cross-Validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b37d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices:\n",
      " [   0    3    5    6   10   11   13 3299 3300 3301]\n",
      "Selected Features:\n",
      " Index(['Age', 'Customer_Support_Interactions', 'Customer_Satisfaction',\n",
      "       'Purchase_Frequency', 'Lifetime_Value', 'Average_Order_Value',\n",
      "       'Number_of_Product_Categories_Purchased',\n",
      "       'Loyalty_Program_Participation_Inactive',\n",
      "       'Engagement_with_Promotions_Low', 'Engagement_with_Promotions_Medium'],\n",
      "      dtype='object')\n",
      "\n",
      "Random Forest\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.9981\n",
      "Confusion Matrix:\n",
      " [[17143    44]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "Training Accuracy: 0.9965\n",
      "Testing Accuracy: 0.9947\n",
      "Confusion Matrix:\n",
      " [[17062   125]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     17187\n",
      "           1       0.98      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           0.99     23776\n",
      "   macro avg       0.99      1.00      0.99     23776\n",
      "weighted avg       0.99      0.99      0.99     23776\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.9972\n",
      "Confusion Matrix:\n",
      " [[17147    40]\n",
      " [   26  6563]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Training Accuracy: 0.9954\n",
      "Testing Accuracy: 0.9926\n",
      "Confusion Matrix:\n",
      " [[17014   173]\n",
      " [    2  6587]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     17187\n",
      "           1       0.97      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           0.99     23776\n",
      "   macro avg       0.99      0.99      0.99     23776\n",
      "weighted avg       0.99      0.99      0.99     23776\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "Training Accuracy: 0.9979\n",
      "Testing Accuracy: 0.9969\n",
      "Confusion Matrix:\n",
      " [[17114    73]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       0.99      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Gradient Boosting\n",
      "Training Accuracy: 0.9988\n",
      "Testing Accuracy: 0.9979\n",
      "Confusion Matrix:\n",
      " [[17137    50]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Training Accuracy: 0.9606\n",
      "Testing Accuracy: 0.9443\n",
      "Confusion Matrix:\n",
      " [[15863  1324]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     17187\n",
      "           1       0.83      1.00      0.91      6589\n",
      "\n",
      "    accuracy                           0.94     23776\n",
      "   macro avg       0.92      0.96      0.93     23776\n",
      "weighted avg       0.95      0.94      0.95     23776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"cleaned_df_luxottica_churn_updated_0108.csv\", index_col=None)\n",
    "\n",
    "# Create a copy of the dataset for transformation\n",
    "dataset_transformed = pd.get_dummies(dataset, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataset_transformed.drop('Churn_Yes', axis=1)\n",
    "y = dataset_transformed['Churn_Yes']\n",
    "\n",
    "# Apply Min-Max scaling to ensure non-negative values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "\n",
    "# Initialize the SelectKBest with chi2\n",
    "select_k_best = SelectKBest(score_func=chi2, k=10)\n",
    "X_selected = select_k_best.fit_transform(X_scaled, y)\n",
    "\n",
    "# Get the selected feature indices and names\n",
    "selected_features_indices = select_k_best.get_support(indices=True)\n",
    "selected_features = X.columns[selected_features_indices]\n",
    "print(\"Selected Feature Indices:\\n\", selected_features_indices)\n",
    "print(\"Selected Features:\\n\", selected_features)\n",
    "\n",
    "# Create the final feature and target datasets with selected features\n",
    "X_final = X[selected_features]\n",
    "y_final = y\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.25, random_state=0)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary to store models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=0),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=0),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(random_state=0),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Train, evaluate each model, and compare training and testing accuracy\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name}\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = model.predict(X_train_resampled)\n",
    "    # Predict on testing data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train_resampled, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print accuracies\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15361dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efc059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab2ef1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
